# â€œâ€â€
Weekly AI News Recap â€” Monday Morning Email

Automated pipeline: RSS feeds â†’ Claude API â†’ email digest.
Runs via GitHub Actions every Monday at 7am EST.

Built by Ryan using AI-assisted development (Claude).
â€œâ€â€

import os
import json
import re
import smtplib
import feedparser
import requests
from datetime import datetime, timedelta
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from anthropic import Anthropic
from dotenv import load_dotenv

load_dotenv()

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ANTHROPIC_API_KEY = os.getenv(â€œANTHROPIC_API_KEYâ€)
SMTP_HOST = os.getenv(â€œSMTP_HOSTâ€, â€œsmtp.gmail.comâ€)
SMTP_PORT = int(os.getenv(â€œSMTP_PORTâ€, 587))
SMTP_USER = os.getenv(â€œSMTP_USERâ€)
SMTP_PASSWORD = os.getenv(â€œSMTP_PASSWORDâ€)  # Gmail: use App Password
EMAIL_TO = os.getenv(â€œEMAIL_TOâ€)
EMAIL_FROM = os.getenv(â€œEMAIL_FROMâ€, SMTP_USER)

# How many days back to look for news

LOOKBACK_DAYS = 7

# â”€â”€ News Sources (30+ feeds across 6 categories) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

RSS_FEEDS = [

```
# â”€â”€ ğŸ¢ Enterprise AI / Salesforce â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{"name": "Salesforce Blog", "url": "https://www.salesforce.com/blog/feed", "category": "salesforce"},
{"name": "Salesforce AI Blog", "url": "https://www.salesforce.com/blog/ai/feed", "category": "salesforce"},
{"name": "Salesforce Developers", "url": "https://developer.salesforce.com/blogs/feed", "category": "salesforce"},
{"name": "Salesforce Engineering", "url": "https://engineering.salesforce.com/feed/", "category": "salesforce"},
{"name": "Microsoft AI Blog", "url": "https://blogs.microsoft.com/ai/feed/", "category": "enterprise_ai"},
{"name": "AWS ML Blog", "url": "https://aws.amazon.com/blogs/machine-learning/feed/", "category": "enterprise_ai"},

# â”€â”€ ğŸ¤– Anthropic / Claude â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{"name": "Anthropic Blog", "url": "https://www.anthropic.com/feed.xml", "category": "anthropic"},

# â”€â”€ ğŸ§ª Frontier AI Labs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{"name": "OpenAI Blog", "url": "https://openai.com/blog/rss.xml", "category": "frontier"},
{"name": "Google AI Blog", "url": "https://blog.google/technology/ai/rss/", "category": "frontier"},
{"name": "Meta AI Blog", "url": "https://ai.meta.com/blog/rss/", "category": "frontier"},
{"name": "DeepMind Blog", "url": "https://deepmind.google/blog/rss.xml", "category": "frontier"},
{"name": "Mistral AI Blog", "url": "https://mistral.ai/feed.xml", "category": "frontier"},

# â”€â”€ ğŸ› ï¸ AI Dev Tools / Vibe Coding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{"name": "HN Best - AI/LLM/Agents", "url": "https://hnrss.org/best?q=AI+OR+LLM+OR+Claude+OR+cursor+OR+agent", "category": "dev_tools"},
{"name": "HN Best - Vibe Coding", "url": "https://hnrss.org/best?q=vibe+coding+OR+agentic+coding+OR+AI+coding", "category": "dev_tools"},
{"name": "GitHub Blog", "url": "https://github.blog/feed/", "category": "dev_tools"},

# â”€â”€ ğŸ“° Major AI News â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{"name": "MIT Tech Review", "url": "https://www.technologyreview.com/feed/", "category": "general_ai"},
{"name": "The Verge - AI", "url": "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml", "category": "general_ai"},
{"name": "Ars Technica", "url": "https://feeds.arstechnica.com/arstechnica/technology-lab", "category": "general_ai"},
{"name": "VentureBeat AI", "url": "https://venturebeat.com/category/ai/feed/", "category": "general_ai"},
{"name": "TechCrunch AI", "url": "https://techcrunch.com/category/artificial-intelligence/feed/", "category": "general_ai"},
{"name": "Wired AI", "url": "https://www.wired.com/feed/tag/ai/latest/rss", "category": "general_ai"},

# â”€â”€ ğŸ’¼ Consulting / Strategy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{"name": "Deloitte AI Insights", "url": "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies.rss.xml", "category": "consulting"},
{"name": "McKinsey AI", "url": "https://www.mckinsey.com/rss/topic/artificial-intelligence.rss", "category": "consulting"},
{"name": "HBR Technology", "url": "https://hbr.org/topic/subject/technology-and-analytics.rss", "category": "consulting"},

# â”€â”€ ğŸ§  Thought Leaders / Substacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{"name": "Simon Willison", "url": "https://simonwillison.net/atom/everything/", "category": "thought_leadership"},
{"name": "Import AI (Jack Clark)", "url": "https://importai.substack.com/feed", "category": "thought_leadership"},
{"name": "The Batch (Andrew Ng)", "url": "https://www.deeplearning.ai/the-batch/feed/", "category": "thought_leadership"},
{"name": "One Useful Thing (Ethan Mollick)", "url": "https://www.oneusefulthing.org/feed", "category": "thought_leadership"},
{"name": "Ahead of AI (Sebastian Raschka)", "url": "https://magazine.sebastianraschka.com/feed", "category": "thought_leadership"},
```

]

# â”€â”€ Fetch News â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_rss_articles(lookback_days: int = LOOKBACK_DAYS) -> list[dict]:
â€œâ€â€œPull articles from all RSS feeds within the lookback window.â€â€â€
cutoff = datetime.now() - timedelta(days=lookback_days)
articles = []

```
for feed_info in RSS_FEEDS:
    try:
        feed = feedparser.parse(feed_info["url"])
        for entry in feed.entries:
            # Parse published date
            published = None
            for date_field in ["published_parsed", "updated_parsed"]:
                if hasattr(entry, date_field) and getattr(entry, date_field):
                    published = datetime(*getattr(entry, date_field)[:6])
                    break

            if published and published < cutoff:
                continue

            title = entry.get("title", "")
            summary = entry.get("summary", entry.get("description", ""))
            summary = re.sub(r"<[^>]+>", "", summary)[:500]

            articles.append({
                "title": title,
                "summary": summary,
                "url": entry.get("link", ""),
                "source": feed_info["name"],
                "category": feed_info["category"],
                "published": published.isoformat() if published else None,
            })
    except Exception as e:
        print(f"âš ï¸  Failed to fetch {feed_info['name']}: {e}")

print(f"ğŸ“° Fetched {len(articles)} articles from {len(RSS_FEEDS)} feeds")
return articles
```

def fetch_newsapi_articles(lookback_days: int = LOOKBACK_DAYS) -> list[dict]:
â€œâ€â€
Optional: Pull from NewsAPI for broader coverage.
Free key at https://newsapi.org (100 req/day).
Set NEWSAPI_KEY in your secrets to enable.
â€œâ€â€
api_key = os.getenv(â€œNEWSAPI_KEYâ€)
if not api_key:
return []

```
cutoff = (datetime.now() - timedelta(days=lookback_days)).strftime("%Y-%m-%d")
queries = [
    "artificial intelligence enterprise",
    "Salesforce AI AgentForce",
    "Anthropic Claude",
    "AI coding tools agents",
]

articles = []
for query in queries:
    try:
        resp = requests.get(
            "https://newsapi.org/v2/everything",
            params={
                "q": query,
                "from": cutoff,
                "sortBy": "relevancy",
                "pageSize": 10,
                "apiKey": api_key,
            },
            timeout=10,
        )
        data = resp.json()
        for a in data.get("articles", []):
            articles.append({
                "title": a.get("title", ""),
                "summary": a.get("description", "")[:500],
                "url": a.get("url", ""),
                "source": a.get("source", {}).get("name", "NewsAPI"),
                "category": "newsapi",
                "published": a.get("publishedAt"),
            })
    except Exception as e:
        print(f"âš ï¸  NewsAPI query failed: {e}")

print(f"ğŸ“° Fetched {len(articles)} articles from NewsAPI")
return articles
```

# â”€â”€ Deduplicate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def deduplicate(articles: list[dict]) -> list[dict]:
â€œâ€â€œRemove duplicate articles by URL and similar titles.â€â€â€
seen_urls = set()
seen_titles = set()
unique = []

```
for a in articles:
    url = a["url"].rstrip("/")
    title_key = a["title"].lower().strip()[:80]

    if url in seen_urls or title_key in seen_titles:
        continue

    seen_urls.add(url)
    seen_titles.add(title_key)
    unique.append(a)

print(f"ğŸ§¹ Deduplicated: {len(articles)} â†’ {len(unique)} articles")
return unique
```

# â”€â”€ Claude Summarization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = â€œâ€â€œYou are an AI news analyst creating a weekly recap email for Ryan,
a senior consultant at Deloitte who works as a functional lead on Salesforce technologies
with a focus on AI (specifically AgentForce). Heâ€™s also into AI dev tools (Cursor, Claude Code,
vibe coding), is a Claude Pro power user, and wants to stay sharp on frontier AI developments.

Your job: Read through this weekâ€™s AI news articles and produce a structured, scannable
Monday morning email digest.

RULES:

- Be direct and opinionated. Donâ€™t just summarize â€” tell Ryan what matters and why.
- If something is relevant to his Salesforce/AgentForce work, flag it explicitly with a ğŸ”µ emoji.
- If something affects his consulting practice or Deloitte, call it out with a ğŸ’¼ emoji.
- If something relates to AI dev tools / vibe coding, flag with a ğŸ› ï¸ emoji.
- Group by theme, not by source.
- Include 5-10 top stories max. Quality over quantity.
- For each story: 1-2 sentence summary + why it matters for Ryan + link.
- End with a â€œğŸ”® What to Watchâ€ section with 2-3 things to keep an eye on next week.
- Keep the tone smart, casual, slightly witty. Not corporate newsletter energy.
- Output clean HTML email format with inline styles only (email-safe, no external CSS).
- Use a clean, modern email layout with good spacing and readability.
- Start with a brief 1-2 sentence â€œTLDRâ€ of the weekâ€™s biggest theme.
  â€œâ€â€

USER_PROMPT_TEMPLATE = â€œâ€â€œHere are {count} AI news articles from the past week ({date_range}).

Please analyze them and create my weekly AI news recap email.

ARTICLES:
{articles_json}
â€œâ€â€

def generate_recap(articles: list[dict]) -> str:
â€œâ€â€œSend articles to Claude and get back a formatted email digest.â€â€â€
client = Anthropic(api_key=ANTHROPIC_API_KEY)

```
# Trim articles to avoid token limits
articles = articles[:60]

date_end = datetime.now().strftime("%B %d, %Y")
date_start = (datetime.now() - timedelta(days=LOOKBACK_DAYS)).strftime("%B %d")

user_prompt = USER_PROMPT_TEMPLATE.format(
    count=len(articles),
    date_range=f"{date_start} â€“ {date_end}",
    articles_json=json.dumps(articles, indent=2, default=str),
)

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=4096,
    system=SYSTEM_PROMPT,
    messages=[{"role": "user", "content": user_prompt}],
)

html_content = response.content[0].text
print(f"âœï¸  Generated recap ({len(html_content)} chars)")
return html_content
```

# â”€â”€ Email â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def send_email(html_content: str):
â€œâ€â€œSend the recap email via SMTP (Gmail-compatible).â€â€â€
today = datetime.now().strftime(â€%B %d, %Yâ€)
subject = fâ€ğŸ¤– Your AI News Recap â€” Week of {today}â€

```
msg = MIMEMultipart("alternative")
msg["Subject"] = subject
msg["From"] = EMAIL_FROM
msg["To"] = EMAIL_TO

# Plain text fallback
plain_text = re.sub(r"<[^>]+>", "", html_content)
msg.attach(MIMEText(plain_text, "plain"))
msg.attach(MIMEText(html_content, "html"))

with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
    server.starttls()
    server.login(SMTP_USER, SMTP_PASSWORD)
    server.sendmail(EMAIL_FROM, EMAIL_TO, msg.as_string())

print(f"ğŸ“§ Email sent to {EMAIL_TO}")
```

def save_local(html_content: str):
â€œâ€â€œSave a local copy for preview/debugging.â€â€â€
os.makedirs(â€œoutputâ€, exist_ok=True)
filename = fâ€output/recap_{datetime.now().strftime(â€™%Y%m%dâ€™)}.htmlâ€
with open(filename, â€œwâ€) as f:
f.write(html_content)
print(fâ€ğŸ’¾ Saved local copy: {filename}â€)

# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
print(â€=â€ * 60)
print(fâ€ğŸš€ AI News Recap â€” {datetime.now().strftime(â€™%A, %B %d %Yâ€™)}â€)
print(â€=â€ * 60)

```
# 1. Fetch from all sources
articles = fetch_rss_articles()
articles += fetch_newsapi_articles()

if not articles:
    print("âŒ No articles found. Check your feeds and internet connection.")
    return

# 2. Deduplicate
articles = deduplicate(articles)

# 3. Generate recap with Claude
html_recap = generate_recap(articles)

# 4. Save locally (always)
save_local(html_recap)

# 5. Send email (if configured)
if SMTP_USER and SMTP_PASSWORD and EMAIL_TO:
    send_email(html_recap)
else:
    print("ğŸ“§ Email not configured â€” check .env file. Local copy saved.")

print("âœ… Done!")
```

if **name** == â€œ**main**â€:
main()
